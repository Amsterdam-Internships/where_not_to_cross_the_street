{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1716834620467
        }
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import laspy\n",
        "from shapely.geometry import Polygon, Point, MultiPolygon, LineString\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import shapely as shp\n",
        "import open3d as o3d\n",
        "import pickle\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.cluster import DBSCAN\n",
        "import re\n",
        "from shapely.ops import transform\n",
        "import pyproj\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1716833997429
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Mount data\n",
        "os.system('sudo blobfuse /home/azureuser/cloudfiles/code/blobfuse/sidewalk --tmp-path=/mnt/resource/blobfusetmp --config-file=/home/azureuser/cloudfiles/code/blobfuse/fuse_connection_sidewalk.cfg -o attr_timeout=3600 -o entry_timeout=3600 -o negative_timeout=3600 -o allow_other -o nonempty')\n",
        "os.system('sudo blobfuse /home/azureuser/cloudfiles/code/blobfuse/ovl --tmp-path=/mnt/resource/blobfusetmp --config-file=/home/azureuser/cloudfiles/code/blobfuse/fuse_connection_ovl.cfg -o attr_timeout=3600 -o entry_timeout=3600 -o negative_timeout=3600 -o allow_other -o nonempty')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1716834460333
        }
      },
      "outputs": [],
      "source": [
        "# Load polygons\n",
        "CW_polygons = gpd.read_file(\"/home/azureuser/cloudfiles/code/blobfuse/sidewalk/processed_data/crossings_project/T2N output/Venserpolder/CW polygons.shp\")\n",
        "CW_polygons = CW_polygons.drop(columns=['FID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1716834688477
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n",
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
            "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
          ]
        }
      ],
      "source": [
        "# Function to transform the data from one coordinate system to another\n",
        "project = pyproj.Transformer.from_proj(\n",
        "    pyproj.Proj(init='epsg:4326'), # source coordinate system\n",
        "    pyproj.Proj(init='epsg:28992')) # destination coordinate system\n",
        "\n",
        "def apply_projection(geometry):\n",
        "    # Your projection transformation code here\n",
        "    transformed_geometry = transform(project.transform, geometry)\n",
        "    return transformed_geometry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform the polygons to RD coordinates\n",
        "CW_polygons['geometry'] = CW_polygons['geometry'].apply(apply_projection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1716833997815
        }
      },
      "outputs": [],
      "source": [
        "# Function to get the PC file names\n",
        "def get_PC_files(folder):\n",
        "    \n",
        "    # Initiate list to save file coordinates\n",
        "    file_list = []\n",
        "    file_names = []\n",
        "\n",
        "    # Get file names\n",
        "    files = os.listdir(folder)\n",
        "\n",
        "    # Pattern to filter out integers\n",
        "    pattern = r'\\d+'\n",
        "\n",
        "    for file_name in files:\n",
        "\n",
        "        # Search for .laz files\n",
        "        match = re.search(\"\\.laz$\", file_name)\n",
        "\n",
        "        if match:\n",
        "            integers = re.findall(pattern, file_name)\n",
        "            file_list.append(integers)\n",
        "            file_names.append(file_name)\n",
        "    \n",
        "    return file_list, file_names  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1716834337474
        }
      },
      "outputs": [],
      "source": [
        "# Insert location point clouds \n",
        "PC_location = \"/home/azureuser/cloudfiles/code/blobfuse/ovl/pointcloud/Unlabeled/Amsterdam/nl-amsd-200923-7415-laz/las_processor_bundled_out\"\n",
        "\n",
        "# Get XY coordinates of PC files and file names\n",
        "PC_XYs, PC_file_names = get_PC_files(PC_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1716834000398
        }
      },
      "outputs": [],
      "source": [
        "def find_PC_files(CWs, PC_XYs, PC_file_names):\n",
        "    CWs_dict = []\n",
        "    PC_list = []\n",
        "\n",
        "    # Loop over all crosswalks\n",
        "    for CW in CWs.iterrows():\n",
        "\n",
        "        # Get the polygon of the crosswalk\n",
        "        polygon = CW[1]['geometry']\n",
        "        \n",
        "        # Get min and max coordinates of polygon\n",
        "        minx, miny, maxx, maxy = polygon.bounds\n",
        "\n",
        "        # Divide by 50 to be in accordance with PC file names\n",
        "        minx_50 = minx/50\n",
        "        miny_50 = miny/50\n",
        "        maxx_50 = maxx/50\n",
        "        maxy_50 = maxy/50\n",
        "\n",
        "        # Round the bounds to find the corresponding PC files\n",
        "        rounded_bounds = [int(minx_50), int(miny_50), int(maxx_50), int(maxy_50)]\n",
        "        \n",
        "        # Find file that corresponds to each bound\n",
        "        minx_miny = \"filtered_\" + str(rounded_bounds[0]) + \"_\" + str(rounded_bounds[1])\n",
        "        minx_maxy = \"filtered_\" + str(rounded_bounds[0]) + \"_\" + str(rounded_bounds[3])\n",
        "        maxx_miny = \"filtered_\" + str(rounded_bounds[2]) + \"_\" + str(rounded_bounds[1])\n",
        "        maxx_maxy = \"filtered_\" + str(rounded_bounds[2]) + \"_\" + str(rounded_bounds[3])\n",
        "        # We create a list of the files\n",
        "        files = [minx_miny, minx_maxy, maxx_maxy, maxx_miny]\n",
        "\n",
        "        # We also need to check if the boundaries are close to the PC border, if this is the case, we add the PC next to it to the PC list\n",
        "        minx_dec = minx_50 - int(minx_50)\n",
        "        miny_dec = miny_50 - int(miny_50)\n",
        "        maxx_dec = maxx_50 - int(maxx_50)\n",
        "        maxy_dec = maxy_50 - int(maxy_50)\n",
        "\n",
        "        minx_low = rounded_bounds[0] - 1\n",
        "        miny_low = rounded_bounds[1] - 1\n",
        "        maxx_high = rounded_bounds[2] + 1\n",
        "        maxy_high = rounded_bounds[3] + 1\n",
        "        \n",
        "\n",
        "        if minx_dec < 0.04:\n",
        "            minx_miny_dec = \"filtered_\" + str(minx_low) + \"_\" + str(rounded_bounds[1])\n",
        "            minx_maxy_dec = \"filtered_\" + str(minx_low) + \"_\" + str(rounded_bounds[3])\n",
        "\n",
        "            files.append(minx_miny_dec)\n",
        "            files.append(minx_maxy_dec)\n",
        "        \n",
        "        if miny_dec < 0.04:\n",
        "            minx_miny_dec = \"filtered_\" + str(rounded_bounds[0]) + \"_\" + str(miny_low)\n",
        "            maxx_miny_dec = \"filtered_\" + str(rounded_bounds[2]) + \"_\" + str(miny_low)\n",
        "\n",
        "            files.append(minx_miny_dec)\n",
        "            files.append(maxx_miny_dec)\n",
        "        \n",
        "        if maxx_dec > 0.96:         \n",
        "            maxx_miny_dec = \"filtered_\" + str(maxx_high) + \"_\" + str(rounded_bounds[1])\n",
        "            maxx_maxy_dec = \"filtered_\" + str(maxx_high) + \"_\" + str(rounded_bounds[3])\n",
        "\n",
        "            files.append(maxx_miny_dec)\n",
        "            files.append(maxx_maxy_dec)\n",
        "\n",
        "        if maxy_dec > 0.96:\n",
        "            minx_maxy_dec = \"filtered_\" + str(rounded_bounds[0]) + \"_\" + str(maxy_high)\n",
        "            maxx_maxy_dec = \"filtered_\" + str(rounded_bounds[2]) + \"_\" + str(maxy_high)\n",
        "\n",
        "            files.append(minx_maxy_dec)\n",
        "            files.append(maxx_maxy_dec)\n",
        "\n",
        "        if minx_dec < 0.04 and maxy_dec > 0.96: \n",
        "            minx_maxy_dec = \"filtered_\" + str(minx_low) + \"_\" + str(maxy_high)\n",
        "\n",
        "            files.append(minx_maxy_dec)\n",
        "\n",
        "        if minx_dec < 0.04 and miny_dec < 0.04:\n",
        "            minx_miny_dec = \"filtered_\" + str(minx_low) + \"_\" + str(miny_low)\n",
        "\n",
        "            files.append(minx_miny_dec)\n",
        "\n",
        "        if maxx_dec > 0.96 and maxy_dec > 0.96:\n",
        "            maxx_maxy_dec = \"filtered_\" + str(maxx_high) + \"_\" + str(maxy_high)\n",
        "\n",
        "            files.append(maxx_maxy_dec)\n",
        "\n",
        "        if maxx_dec > 0.96 and miny_dec < 0.04:\n",
        "            maxx_miny_dec = \"filtered_\" + str(maxx_high) + \"_\" + str(miny_low)\n",
        "\n",
        "            files.append(maxx_miny_dec)\n",
        "\n",
        "        # Finally we create a set of the list of files to prevent duplicates\n",
        "        files = list(set(files))\n",
        "               \n",
        "        CW_dict = {\n",
        "            'CW_index': CW[0],\n",
        "            'CW_polygon': polygon,\n",
        "            'PC_list': files\n",
        "        }\n",
        "\n",
        "        CWs_dict.append(CW_dict)\n",
        "        PC_list.extend(files)\n",
        "    \n",
        "    PC_list = list(set(PC_list))\n",
        "\n",
        "    return CWs_dict, PC_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1716834723573
        }
      },
      "outputs": [],
      "source": [
        "# Load polygons in a dictionary and a list of the necessary PC files\n",
        "CWs, PC_list = find_PC_files(CW_polygons, PC_XYs, PC_file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1716834958474
        }
      },
      "outputs": [],
      "source": [
        "# Function to load the PC files\n",
        "def load_PCs(PC_list, folder):\n",
        "    PCs = []\n",
        "    \n",
        "    for pc_name in PC_list:\n",
        "        file = os.path.join(folder, pc_name + \".laz\")\n",
        "        if os.path.exists(file):\n",
        "            laz_file = laspy.read(file)\n",
        "            name = pc_name.split(\".\")[0]\n",
        "            PC_coords = laz_file.xyz\n",
        "            PC_intensity = laz_file.intensity\n",
        "\n",
        "            PCs.append({\"name\": name, \"laz_file\": laz_file, \"PC_coords\": PC_coords, \"PC_intensity\": PC_intensity})\n",
        "           \n",
        "    return PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1716835165406
        }
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the PC files into a dictionary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m PCs \u001b[38;5;241m=\u001b[39m \u001b[43mload_PCs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPC_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPC_location\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mload_PCs\u001b[0;34m(PC_list, folder)\u001b[0m\n\u001b[1;32m      8\u001b[0m laz_file \u001b[38;5;241m=\u001b[39m laspy\u001b[38;5;241m.\u001b[39mread(file)\n\u001b[1;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m pc_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m PC_coords \u001b[38;5;241m=\u001b[39m \u001b[43mlaz_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxyz\u001b[49m\n\u001b[1;32m     11\u001b[0m PC_intensity \u001b[38;5;241m=\u001b[39m laz_file\u001b[38;5;241m.\u001b[39mintensity\n\u001b[1;32m     13\u001b[0m PCs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaz_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: laz_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPC_coords\u001b[39m\u001b[38;5;124m\"\u001b[39m: PC_coords, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPC_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m: PC_intensity})\n",
            "File \u001b[0;32m/anaconda/envs/tile2net/lib/python3.11/site-packages/laspy/lasdata.py:82\u001b[0m, in \u001b[0;36mLasData.xyz\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mxyz\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a **new** 2D numpy array with the x,y,z coordinates\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    >>> import laspy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose()\n",
            "File \u001b[0;32m/anaconda/envs/tile2net/lib/python3.11/site-packages/laspy/point/dims.py:567\u001b[0m, in \u001b[0;36mArrayView.__array_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_function__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, types, args, kwargs):\n\u001b[1;32m    566\u001b[0m     argslist \u001b[38;5;241m=\u001b[39m _convert_array_views_to_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, args)\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margslist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load the PC files into a dictionary\n",
        "PCs = load_PCs(PC_list, PC_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to check if the PCs necessary for each polygon are present, if not, remove the polygon\n",
        "def check_CW_PC(CWs, PCs):\n",
        "\n",
        "    PC_names = []\n",
        "\n",
        "    # Loop over PCs dictionary and save PC names\n",
        "    for PC in PCs:\n",
        "        PC_names.append(PC['name'])\n",
        "\n",
        "    # Loop over CWs and check if the PCs exist\n",
        "\n",
        "    for CW in CWs:\n",
        "\n",
        "        for PC in CW['PC_list']:\n",
        "            \n",
        "            if PC not in PC_names:\n",
        "\n",
        "                CW['PC_list'] = CW['PC_list'].remove(PC)\n",
        "    \n",
        "    # Remove CWs that have no PCs as they are outside the targeted area\n",
        "    filtered_CWs = []\n",
        "\n",
        "    for CW_check in CWs:\n",
        "\n",
        "        if CW_check['PC_list'] is not None:\n",
        "            filtered_CWs.append(CW_check)   \n",
        "        else: print(\"Removed CW with index: \", CW_check['CW_index'])\n",
        "    \n",
        "    return filtered_CWs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed CW with index:  3\n"
          ]
        }
      ],
      "source": [
        "CWs_filtered = check_CW_PC(CWs, PCs)\n",
        "del CWs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Down sampling point clouds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1716835209442
        }
      },
      "outputs": [],
      "source": [
        "# Function to cut the upper layer of the PCs\n",
        "def cut_PC(pc):\n",
        "    # Find coordinates below threshold\n",
        "    indices = np.where(pc['PC_coords'][:, 2] < 1)\n",
        "    z = pc['PC_coords'][:, 2]\n",
        "    average_z = np.mean(z)\n",
        "\n",
        "    # Cut intensity accordingly\n",
        "    pc['PC_intensity_low'] = pc['PC_intensity'][indices]\n",
        "    pc['PC_coords_low'] = pc['PC_coords'][indices]\n",
        "\n",
        "    # Compute average and std z\n",
        "    z = pc['PC_coords_low'][:, 2]\n",
        "    average_z = np.mean(z)\n",
        "    sd_z = np.std(z)\n",
        "    threshold = average_z + sd_z\n",
        "\n",
        "    # Compute points above threshold\n",
        "    indices_thres = np.where(pc['PC_coords_low'][:, 2] < threshold)\n",
        "    pc['PC_intensity_low'] =  pc['PC_intensity_low'][indices_thres] \n",
        "    pc['PC_coords_low'] =  pc['PC_coords_low'][indices_thres] \n",
        "\n",
        "    return pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1716835212449
        }
      },
      "outputs": [],
      "source": [
        "# Function to downsample the PCs\n",
        "def down_sample_PC(pc, coords, intensity_string):\n",
        "    xyz = pc[coords]\n",
        "    intensity = pc[intensity_string]\n",
        "    \n",
        "    # Convert to Open3D point cloud using only XYZ\n",
        "    pc_o3d = o3d.geometry.PointCloud()\n",
        "    pc_o3d.points = o3d.utility.Vector3dVector(xyz)\n",
        "\n",
        "    # Perform voxel downsampling\n",
        "    downsampled_pc_o3d = pc_o3d.voxel_down_sample(0.02)\n",
        "\n",
        "    # Retrieve downsampled XYZ points\n",
        "    pc[coords + \"_ds\"] = np.asarray(downsampled_pc_o3d.points) \n",
        "\n",
        "    # Create a KDTree for the original point cloud\n",
        "    tree = cKDTree(xyz)\n",
        "\n",
        "    # For each downsampled point, find its nearest neighbor in the original cloud\n",
        "    _, indices = tree.query(pc[coords + '_ds'])\n",
        "\n",
        "    # Get indices of intensity\n",
        "    pc[intensity_string + \"_ds\"] = intensity[indices]\n",
        "\n",
        "    return pc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1716835216622
        }
      },
      "outputs": [],
      "source": [
        "# Function to process the PCs\n",
        "def cut_ds_PC(PCs, coord_string, intensity_string):\n",
        "\n",
        "    for pc in PCs:\n",
        "        pc = cut_PC(pc)\n",
        "        pc = down_sample_PC(pc, coord_string, intensity_string)\n",
        "    \n",
        "    return PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1716835651343
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/anaconda/envs/tile2net/lib/python3.11/site-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PCs_cut \u001b[38;5;241m=\u001b[39m \u001b[43mcut_ds_PC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPCs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPC_coords_low\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPC_intensity_low\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m PCs\n",
            "Cell \u001b[0;32mIn[49], line 6\u001b[0m, in \u001b[0;36mcut_ds_PC\u001b[0;34m(PCs, coord_string, intensity_string)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pc \u001b[38;5;129;01min\u001b[39;00m PCs:\n\u001b[1;32m      5\u001b[0m     pc \u001b[38;5;241m=\u001b[39m cut_PC(pc)\n\u001b[0;32m----> 6\u001b[0m     pc \u001b[38;5;241m=\u001b[39m \u001b[43mdown_sample_PC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PCs\n",
            "Cell \u001b[0;32mIn[48], line 11\u001b[0m, in \u001b[0;36mdown_sample_PC\u001b[0;34m(pc, coords, intensity_string)\u001b[0m\n\u001b[1;32m      8\u001b[0m pc_o3d\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(xyz)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Perform voxel downsampling\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m downsampled_pc_o3d \u001b[38;5;241m=\u001b[39m \u001b[43mpc_o3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxel_down_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Retrieve downsampled XYZ points\u001b[39;00m\n\u001b[1;32m     14\u001b[0m pc[coords \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(downsampled_pc_o3d\u001b[38;5;241m.\u001b[39mpoints) \n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Downsample and cut the polygons based on height\n",
        "PCs_cut = cut_ds_PC(PCs, \"PC_coords_low\", \"PC_intensity_low\")\n",
        "del PCs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "gather": {
          "logged": 1716834001792
        }
      },
      "outputs": [],
      "source": [
        "# Function to plot CW\n",
        "def plot_PC_2D(PC_pol_dict, coords, intensity):\n",
        "    x = PC_pol_dict[coords][:, 0]\n",
        "    y = PC_pol_dict[coords][:, 1]\n",
        "    plt.figure()\n",
        "    plt.scatter(x, y, c=PC_pol_dict[intensity], cmap='viridis')\n",
        "    plt.colorbar(label='Reflective index')  # Add colorbar to show gradient values\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title('2D with intensity gradient')\n",
        "    plt.grid(True)\n",
        "    plt.axis('equal')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matching CW polygons to PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "gather": {
          "logged": 1716835752436
        }
      },
      "outputs": [],
      "source": [
        "# Cut point clouds based on polygon coordinate\n",
        "def PC_pol_match(PC, pol):\n",
        "\n",
        "    # Get the bounding box (rectangle) of the polygon\n",
        "    minx, miny, maxx, maxy = pol['CW_polygon'].bounds\n",
        "    \n",
        "    # Determine condition based on polygon bounds\n",
        "    condition = ((PC['PC_coords_low_ds'][:, 0] > minx) & (PC['PC_coords_low_ds'][:, 0] < maxx) \n",
        "                &  (PC['PC_coords_low_ds'][:, 1] > miny) & (PC['PC_coords_low_ds'][:, 1] < maxy))\n",
        "\n",
        "    # Apply condition to get indices\n",
        "    indexes = np.where(condition)\n",
        "\n",
        "    # Check if any matches were found\n",
        "    if len(indexes[0]) > 0:\n",
        "        \n",
        "        # Apply indexing to coordinates and intensity\n",
        "        intensity = PC['PC_intensity_low_ds'][indexes]\n",
        "        coords = PC['PC_coords_low_ds'][indexes]\n",
        "\n",
        "        return {'CW_index': pol['CW_index'], 'polygon': pol['CW_polygon'], 'PC_file': [PC['name']], 'PC_coords_low_ds': coords, 'PC_intensity_low_ds': intensity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "gather": {
          "logged": 1716835759452
        }
      },
      "outputs": [],
      "source": [
        "# Function to merge polygons that are spread over two point clouds\n",
        "def merge_matches(match1, match2):\n",
        "\n",
        "    # Concatenate coordinates and intensity of both PC files belonging to the same polygon \n",
        "    coords = np.vstack((match1['PC_coords_low_ds'], (match2['PC_coords_low_ds'])))\n",
        "    intensity = np.hstack((match1['PC_intensity_low_ds'], (match2['PC_intensity_low_ds'])))\n",
        "    \n",
        "    # Create list of PC files to add to dictionary \n",
        "    PC_list = match1['PC_file'] + match2['PC_file']\n",
        "    \n",
        "    # Create dictionary for matched point clouds\n",
        "    new_match = {'CW_index': match1['CW_index'], 'polygon': match1['polygon'], 'PC_file': PC_list, 'PC_coords_low_ds': coords, 'PC_intensity_low_ds': intensity}\n",
        "    \n",
        "    return new_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "gather": {
          "logged": 1716835765436
        }
      },
      "outputs": [],
      "source": [
        "# Function to group PC matches of the same polygon together\n",
        "def group_matches(all_matches):\n",
        "    # Create list to group together polygons that are spread over multiple point clouds\n",
        "    grouped_data = []\n",
        "\n",
        "    # Create a deep copy of the previously identified matches\n",
        "    match_copy = copy.deepcopy(all_matches)\n",
        "\n",
        "    # Loop over all matches\n",
        "    for item in match_copy:\n",
        "\n",
        "        index = item['CW_index']\n",
        "\n",
        "        found = False\n",
        "\n",
        "        for sublist in grouped_data:\n",
        "\n",
        "            # Check if the polygon is already in the list and append to the corresponding list item if this is the case\n",
        "            if sublist and sublist[0]['CW_index'] == index:\n",
        "                sublist.append(item)\n",
        "                found = True\n",
        "                break\n",
        "            \n",
        "        # If the polygon is not already in the list, append it \n",
        "        if not found:\n",
        "            grouped_data.append([item])\n",
        "    \n",
        "    return grouped_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "gather": {
          "logged": 1716835769422
        }
      },
      "outputs": [],
      "source": [
        "# Function to process PC matches of the same polygon\n",
        "def process_grouped_matches(grouped_data):\n",
        "    # Loop over the grouped polygons\n",
        "    for group in grouped_data:\n",
        "\n",
        "        # Check if there is multiple PC files for one polygon \n",
        "        if len(group) > 1:\n",
        "\n",
        "            # Loop over each item except the last one\n",
        "            for i in range(len(group) - 1):\n",
        "\n",
        "                # Merge the first item with the next one and replace the first item \n",
        "                match = merge_matches(group[0], group[1])\n",
        "                \n",
        "                group[0] = match\n",
        "\n",
        "                group.pop(1)\n",
        "\n",
        "    # Flatten the grouped data list as each list item only has one item now\n",
        "    grouped_data_flat = [item for sublist in grouped_data for item in sublist]\n",
        "\n",
        "    return grouped_data_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "gather": {
          "logged": 1716835776664
        }
      },
      "outputs": [],
      "source": [
        "# Function to match PCs and polygons\n",
        "def match_PC_pol(CW_polygons, PCs):\n",
        "\n",
        "    # Create list to save all matches found\n",
        "    all_matches = []\n",
        "\n",
        "    # Loop over all polygons\n",
        "    for index in range(0, len(CW_polygons)):\n",
        "        cw = CW_polygons[index]\n",
        "        #print(cw['PC_list'])\n",
        "\n",
        "        for pc in PCs:\n",
        "            if pc['name'] in cw['PC_list']:\n",
        "                match = PC_pol_match(pc, cw)\n",
        "                if match:\n",
        "                    all_matches.append(match)\n",
        "    \n",
        "    grouped_data = group_matches(all_matches)\n",
        "    merged_data = process_grouped_matches(grouped_data)\n",
        "    \n",
        "    return merged_data, grouped_data, all_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "gather": {
          "logged": 1716834001899
        }
      },
      "outputs": [],
      "source": [
        "# Find the PC data that matches the polygons\n",
        "merged_data, grouped_data, all_matches = match_PC_pol(CWs, PCs_cut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Growing polygons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To filter the polygons, we take several steps.\n",
        "1. Cluster areas within a polygon with a high intensity value in close proximity together \n",
        "2. For each cluster, check the surrounding points. If points have a high intensity value, add them to the cluster.\n",
        "\n",
        "Step 2 is repeated untill there are no more surrounding points with a high intensity value. This way, only areas that are present in the original polygon are grown and outside areas are not included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "gather": {
          "logged": 1716834001928
        }
      },
      "outputs": [],
      "source": [
        "# Function to filter the buffered layer\n",
        "def filter_buffer(buffer):\n",
        "    buffer_copy = copy.deepcopy(buffer)\n",
        "\n",
        "    # Determine condition based on polygon bounds\n",
        "    condition = (buffer_copy['intensity'] > 30000)\n",
        "\n",
        "    # Apply condition to get indices\n",
        "    indexes = np.where(condition)\n",
        "    \n",
        "    # Check if any matches were found\n",
        "    if len(indexes[0]) > 0:\n",
        "        \n",
        "        # Apply indexing to coordinates and intensity\n",
        "        buffer_copy['intensity_filtered'] = np.array(buffer_copy['intensity'][indexes[0]])\n",
        "        buffer_copy['coords_filtered'] = np.array(buffer_copy['coords'][indexes[0]])\n",
        "        \n",
        "    return buffer_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "gather": {
          "logged": 1716834001949
        }
      },
      "outputs": [],
      "source": [
        "# Function to filter the PC points in a polygon based on the intensity values of the points\n",
        "def filter_intensity(cw, min_intensity):\n",
        "    return_cw = copy.deepcopy(cw)\n",
        "\n",
        "    # Determine condition based on polygon bounds\n",
        "    condition = (return_cw['PC_intensity_low_ds'] > min_intensity)\n",
        "\n",
        "    # Apply condition to get indices\n",
        "    indexes = np.where(condition)\n",
        "\n",
        "    # Check if any matches were found\n",
        "    if len(indexes[0]) > 0:\n",
        "        \n",
        "        # Apply indexing to coordinates and intensity\n",
        "        return_cw['PC_intensity_low_ds_filtered'] = return_cw['PC_intensity_low_ds'][indexes]\n",
        "        return_cw['PC_coords_low_ds_filtered'] = return_cw['PC_coords_low_ds'][indexes]\n",
        "       \n",
        "\n",
        "        return return_cw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "gather": {
          "logged": 1716834001966
        }
      },
      "outputs": [],
      "source": [
        "# Function to cluster the polygon into clusters of points that have a high intensity and are close together in space\n",
        "def cluster_pol(CW):\n",
        "    # Create list to save the clusters that are found\n",
        "    cluster_list = []\n",
        "\n",
        "    # Filter the original polygon to only include points with a high intensity\n",
        "    filtered = filter_intensity(CW, 30000)\n",
        "    \n",
        "\n",
        "    if filtered:\n",
        "    \n",
        "        # Use DBSCAN to cluster the points in the polygon\n",
        "        dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "        dbscan.fit(filtered['PC_coords_low_ds_filtered'])\n",
        "\n",
        "        # Get labels created by DBSCAN\n",
        "        labels = dbscan.labels_\n",
        "\n",
        "        # Create dictionary to save clusters\n",
        "        cluster_data = {}\n",
        "\n",
        "        # Loop over each point in the filtered polygon and check to which cluster it belongs\n",
        "        # Group coordinates and intensity values based on their label in the cluster_data dictionary\n",
        "        for label, point, value in zip(labels, filtered['PC_coords_low_ds_filtered'], filtered['PC_intensity_low_ds_filtered']):\n",
        "            if label not in cluster_data:\n",
        "                cluster_data[label] = {'coords': [], 'intensity': []}  \n",
        "            cluster_data[label]['coords'].append(point)\n",
        "            cluster_data[label]['intensity'].append(value)\n",
        "        \n",
        "        # Transform the coordinates and intensity values to np arrays to make them easier to work with\n",
        "        for label in np.unique(labels):\n",
        "            cluster_data[label]['coords'] = np.array(cluster_data[label]['coords'])\n",
        "            cluster_data[label]['intensity'] = np.array(cluster_data[label]['intensity'])\n",
        "\n",
        "        # Loop over the created clusters and save them in the cluster_list\n",
        "        for cluster in cluster_data:\n",
        "            cluster_dict = {}\n",
        "\n",
        "            # Only keep clusters that are over 100 points to pre-emptively filter out noise\n",
        "            if (len(cluster_data[cluster]['coords']) > 50):\n",
        "\n",
        "                # Save cluster in a similar manner as the original polygon\n",
        "                cluster_dict['CW_index'] = CW['CW_index']\n",
        "                cluster_dict['PC_file'] = CW['PC_file']\n",
        "                cluster_dict['coordinates'] = cluster_data[cluster]['coords']\n",
        "                cluster_dict['intensity'] = cluster_data[cluster]['intensity']\n",
        "                \n",
        "                cluster_list.append(cluster_dict)\n",
        "        \n",
        "        return cluster_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "gather": {
          "logged": 1716834002001
        }
      },
      "outputs": [],
      "source": [
        "# Function to grow the clusters based on creating a buffer around them and seeing if there are points in the buffer with a high intensity\n",
        "def grow_cluster(PC_coords, PC_intensity, cluster_coords):\n",
        "\n",
        "    # Initialize an empty list to keep track of the points that are added to the clusters\n",
        "    added = []\n",
        "\n",
        "    # Build a KDTree for fast nearest neighbor search\n",
        "    tree = cKDTree(PC_coords)\n",
        "\n",
        "    # Define the radius within which points are considered neighbors\n",
        "    radius = 0.12\n",
        "\n",
        "    # Initialize the starting coordinates for the cluster growth as the original cluster\n",
        "    coords = cluster_coords\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # Find indices of neighbors within the specified radius\n",
        "        neighbor_indices = tree.query_ball_point(coords, radius)\n",
        "\n",
        "        # Initialize a list to store unique inidces of new points to add\n",
        "        indices = []\n",
        "\n",
        "        # Iterate through the neighbor indices to see if points have already been added\n",
        "        for index in neighbor_indices:\n",
        "            for i in index:\n",
        "                # Add them if this is not the case\n",
        "                if i not in added:\n",
        "                    indices.append(i)\n",
        "                    added.append(i)\n",
        "\n",
        "        # Remove duplicates from the lists of indices\n",
        "        indices = list(set(indices))\n",
        "        added = list(set(added))\n",
        "\n",
        "        # If no new points are found, exit the loop\n",
        "        if len(indices) == 0:\n",
        "            break\n",
        "\n",
        "        # Retrieve coordinates and intensities of the neighboring points\n",
        "        neighbor_coords = PC_coords[indices]\n",
        "        neighbor_intensities = PC_intensity[indices]\n",
        "\n",
        "        # Store the neighbors in a temporary dictionary\n",
        "        temp = {'coords': neighbor_coords, 'intensity': neighbor_intensities}\n",
        "\n",
        "        # Apply a filtering function to the temporary dictionary to only keep points with a high intensity\n",
        "        temp_filtered = filter_buffer(temp)\n",
        "\n",
        "        # If new filtered coordinates are available, update the coordinates for the next iteration\n",
        "        if 'coords_filtered' in temp_filtered:\n",
        "            coords = temp_filtered['coords_filtered']\n",
        "\n",
        "    # Extract the final cluster coordinates and intensities\n",
        "    cluster_coords = PC_coords[added]\n",
        "    cluster_intensity = PC_intensity[added]\n",
        "\n",
        "    # Store the final cluster information in a dictionary\n",
        "    final = {'coords': cluster_coords, 'intensity': cluster_intensity}\n",
        "\n",
        "    # Apply filtering to the final cluster\n",
        "    final = filter_buffer(final)\n",
        "    \n",
        "    # Return the filtered final cluster\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "gather": {
          "logged": 1716834002021
        }
      },
      "outputs": [],
      "source": [
        "# Function to create and process the PC polygon points\n",
        "def get_clusters(polygon, PCs, PC_coords_string, PC_intensity_string): \n",
        "\n",
        "    # Initialize coordinate and intensity array\n",
        "    PC_coords_temp = []\n",
        "    PC_intensity_temp = []\n",
        "\n",
        "    # Get PC file that corresponds to that of the original polygon\n",
        "    for PC_name in polygon['PC_file']:\n",
        "        PC = list(filter(lambda PC: PC['name'] == PC_name, PCs))\n",
        "        \n",
        "        sub_PC_coords = PC[0][PC_coords_string]\n",
        "        sub_PC_intensity = PC[0][PC_intensity_string]\n",
        "        \n",
        "        PC_coords_temp.append(sub_PC_coords)\n",
        "        PC_intensity_temp.append(sub_PC_intensity)\n",
        "    \n",
        "    \n",
        "    PC_coords = np.concatenate(PC_coords_temp, axis=0)\n",
        "    PC_intensity = np.concatenate(PC_intensity_temp, axis=0)\n",
        "\n",
        "    # Get clusters from polygon\n",
        "    cluster_dict = cluster_pol(polygon)\n",
        "\n",
        "    if cluster_dict:\n",
        "\n",
        "        # For each found cluster, grow it and update the cluster data\n",
        "        for cluster in cluster_dict:\n",
        "            clean_cluster = grow_cluster(PC_coords, PC_intensity, cluster['coordinates'])\n",
        "            if 'coords_filtered' in clean_cluster:\n",
        "                cluster['clean_coords'] = clean_cluster['coords_filtered']\n",
        "                cluster['clean_intensity'] = clean_cluster['intensity_filtered']\n",
        "        \n",
        "        # Return cluster dictionary\n",
        "        return cluster_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "gather": {
          "logged": 1716834002040
        }
      },
      "outputs": [],
      "source": [
        "# Function to merge polygons that are spread over two point clouds\n",
        "def merge_clusters(cluster_list):\n",
        "\n",
        "    # Initialize arrays final cw\n",
        "    CW_name = []\n",
        "    PC_file = []\n",
        "    PC_coords= []\n",
        "    PC_intensity = []\n",
        "    PC_coords_clean = []\n",
        "    PC_intensity_clean = []\n",
        "\n",
        "    # Get PC file that corresponds to that of the original polygon++++++++\n",
        "    \n",
        "    for cluster in cluster_list:\n",
        "       \n",
        "        CW_name.append(cluster['CW_index'])\n",
        "        PC_file.append(cluster['PC_file'])\n",
        "        PC_coords.append(cluster['coordinates'])\n",
        "        PC_intensity.append(cluster['intensity'])\n",
        "\n",
        "        if 'clean_coords' in cluster:\n",
        "            PC_coords_clean.append(cluster['clean_coords'])\n",
        "            PC_intensity_clean.append(cluster['clean_intensity'])\n",
        "    \n",
        "    PC_file = np.concatenate(PC_file, axis=0)\n",
        "    PC_coords = np.concatenate(PC_coords, axis=0)\n",
        "    PC_intensity = np.concatenate(PC_intensity, axis=0)\n",
        "    PC_coords_clean = np.concatenate(PC_coords_clean, axis=0)\n",
        "    PC_intensity_clean = np.concatenate(PC_intensity_clean, axis=0)\n",
        "\n",
        "    cw = {}\n",
        "\n",
        "    cw['CW_index'] = list(set(CW_name))\n",
        "    cw['PC_file'] = list(set(PC_file))\n",
        "    cw['coordinates'] = PC_coords\n",
        "    cw['intensity'] = PC_intensity\n",
        "    cw['coordinates_clean'] = PC_coords_clean\n",
        "    cw['intensity_clean'] = PC_intensity_clean\n",
        "       \n",
        "    return cw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716834002060
        }
      },
      "outputs": [],
      "source": [
        "# Cluster the PC polygons and grow them to get complete road markings\n",
        "final = []\n",
        "for merge in merged_data:\n",
        "    cluster_dict = get_clusters(merge, PCs_cut, 'PC_coords_low_ds', 'PC_intensity_low_ds')\n",
        "    if cluster_dict:\n",
        "        final.append(cluster_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1716834002078
        }
      },
      "outputs": [],
      "source": [
        "# Save the dictionary with the PC polygon clusters\n",
        "path = \"/home/azureuser/cloudfiles/code/blobfuse/sidewalk/processed_data/crossings_project/CW cleaning/Venserpolder/clusterdicts Venserpolder.pkl\"\n",
        "\n",
        "with open(path, 'wb') as file:\n",
        "    pickle.dump(final, file)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "tile2net"
    },
    "kernelspec": {
      "display_name": "tile2net",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

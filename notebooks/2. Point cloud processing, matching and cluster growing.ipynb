{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1716834620467
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
            "[Open3D INFO] WebRTC GUI backend enabled.\n",
            "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
          ]
        }
      ],
      "source": [
        "import geopandas as gpd\n",
        "import laspy\n",
        "import numpy as np\n",
        "import copy\n",
        "import open3d as o3d\n",
        "import pickle\n",
        "from scipy.spatial import cKDTree\n",
        "from sklearn.cluster import DBSCAN\n",
        "import re\n",
        "from interruptingcow import timeout\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this notebook serves as both step 2 and step 4 of the framework. This notebook is applied after running notebook 1 and another time after running notebook 3 but with different input polygons. The required input is desribed at the end of the notebook overview. Do not forget to set the origin of the polygons in the settings cell that prompts you to do so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook overview\n",
        "This notebook completes three different step. In order to not have to save the really large point cloud files and carry them over to another notebook, the steps are combined into one notebook.\n",
        "\n",
        "### Step 1\n",
        "Firstly, we determine which point cloud files are required. The point clouds are cut into 50 by 50 meter squares and saved based on their coordinates. By looking at the coordinates of each crosswalk polygon in our data, we can determine in which point cloud file(s) they fall. As we sometimes need to grow elements of the crosswalk polygon to capture the entire crosswalk, we add bordering point cloud files for crosswalks that are close to the border of a point cloud file. We filter for the files that we need in order to not have to process all point cloud data, which would take a lot of time and computer power.\n",
        "\n",
        "### Step 2\n",
        "Once we know which point cloud files we will need to process the polygons, we continue by cutting and dowsnampling them. We cut the point clouds to only contain points on the ground as these are the only relevant ones for the task at hand. Furthermore, we downsample the point clouds in order to save computational resources and time, as the point cloud files are rather large.\n",
        "\n",
        "### Step 3\n",
        "When the point clouds are ready, we can match the crosswalk polygons to the point cloud data. We do this by creating a dictionary and adding an item for each crosswalk. This dictionary will contain all the point cloud points that fall within the polygon. \n",
        "\n",
        "### Step 4\n",
        "Finally, we will process the point cloud data within each polygon. We first filter out all points that have an intensity value below a certain threshold as these are likely not part of road markings which have a large intensity value. The remaining points within a polygon will be clustered. This is done in order to isolate individual road markings (such as a signle crosswalk stripe). Each resulting cluster will be processed in order to capture a full feature, as sometimes Tile2Net only captures part of a crosswalk stripe. This is done by iteratively analyzing all bordering points of a cluster and adding the ones that exceed the intensity threshold, as these points are likely part of the same feature. This iterative process is repeated until no bordering points exceed the intensity threshold. This step results in a list of lists where each sub-list contains dictionaries of processed clusters that stem from the same original polygon.\n",
        "\n",
        "### If coming from notebook 1\n",
        "**Input**: \n",
        "- Shapely file with crosswalk polygons in area of interest.\n",
        "- Location of folder that contains point cloud files. \n",
        "\n",
        "**Output**: \n",
        "- List of lists in which each sub-list contains dictionaries of processed clusters that stem from the same original polygon.\n",
        "\n",
        "**Previous notebook**: 1. T2N processing.\n",
        "\n",
        "**Next notebook**: 3. Polygon analysis + growing.\n",
        "\n",
        "### If coming from notebook 3\n",
        "\n",
        "**Input**: \n",
        "- Shapely file with extensions of processed crosswalk polygons.\n",
        "- Location of folder that contains point cloud files. \n",
        "\n",
        "**Output**: \n",
        "- List of lists in which each sub-list contains dictionaries of processed clusters that stem from the same extension polygon as created in notebook 3.\n",
        "\n",
        "**Previous notebook**: 3. Polygon analysis + growing.\n",
        "\n",
        "**Next notebook**: 4. Extended polygon filtering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set CRS\n",
        "CRS = 'epsg:28992'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine whether you are running the code for the Tile2Net polygons (created in notebook 1) or the extension polygons (created in notebook 3)\n",
        "# Choose \"T2N\" if coming from notebook 1 and \"extension\" if coming from notebook 3\n",
        "polygon_origin = \"T2N\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1716834460333
        }
      },
      "outputs": [],
      "source": [
        "# Load crosswalk polygons\n",
        "if polygon_origin == \"T2N\":\n",
        "    CW_polygons = gpd.read_file(\"../data/output/crosswalk polygons Tile2Net.shp\")\n",
        "if polygon_origin == \"extension\":\n",
        "    CW_polygons = gpd.read_file(\"../data/output/extended polygons.shp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1716833997815
        }
      },
      "outputs": [],
      "source": [
        "# Function to get the PC file names\n",
        "def get_PC_files(folder):\n",
        "    \n",
        "    # Initiate list to save file coordinates\n",
        "    file_list = []\n",
        "    file_names = []\n",
        "\n",
        "    # Get file names\n",
        "    files = os.listdir(folder)\n",
        "\n",
        "    # Pattern to filter out integers\n",
        "    pattern = r'\\d+'\n",
        "\n",
        "    for file_name in files:\n",
        "\n",
        "        # Search for .laz files\n",
        "        match = re.search(\"\\.laz$\", file_name)\n",
        "\n",
        "        if match:\n",
        "            integers = re.findall(pattern, file_name)\n",
        "            file_list.append(integers)\n",
        "            file_names.append(file_name)\n",
        "    \n",
        "    return file_list, file_names  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1716834337474
        }
      },
      "outputs": [],
      "source": [
        "# Insert location point clouds \n",
        "PC_location = \"../data/input/point clouds\"\n",
        "\n",
        "# Get XY coordinates and file names of the point clouds\n",
        "PC_XYs, PC_file_names = get_PC_files(PC_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_PC_files(CWs, PC_XYs, PC_file_names):\n",
        "    def generate_file_name(x, y):\n",
        "        return f\"filtered_{x}_{y}\"\n",
        "    \n",
        "    def add_adjacent_files(files, rounded_bounds, minx_dec, miny_dec, maxx_dec, maxy_dec):\n",
        "        minx_low = rounded_bounds[0] - 1\n",
        "        miny_low = rounded_bounds[1] - 1\n",
        "        maxx_high = rounded_bounds[2] + 1\n",
        "        maxy_high = rounded_bounds[3] + 1\n",
        "\n",
        "        if minx_dec < 0.04:\n",
        "            files.append(generate_file_name(minx_low, rounded_bounds[1]))\n",
        "            files.append(generate_file_name(minx_low, rounded_bounds[3]))\n",
        "\n",
        "        if miny_dec < 0.04:\n",
        "            files.append(generate_file_name(rounded_bounds[0], miny_low))\n",
        "            files.append(generate_file_name(rounded_bounds[2], miny_low))\n",
        "\n",
        "        if maxx_dec > 0.96:\n",
        "            files.append(generate_file_name(maxx_high, rounded_bounds[1]))\n",
        "            files.append(generate_file_name(maxx_high, rounded_bounds[3]))\n",
        "\n",
        "        if maxy_dec > 0.96:\n",
        "            files.append(generate_file_name(rounded_bounds[0], maxy_high))\n",
        "            files.append(generate_file_name(rounded_bounds[2], maxy_high))\n",
        "\n",
        "        if minx_dec < 0.04 and maxy_dec > 0.96:\n",
        "            files.append(generate_file_name(minx_low, maxy_high))\n",
        "\n",
        "        if minx_dec < 0.04 and miny_dec < 0.04:\n",
        "            files.append(generate_file_name(minx_low, miny_low))\n",
        "\n",
        "        if maxx_dec > 0.96 and maxy_dec > 0.96:\n",
        "            files.append(generate_file_name(maxx_high, maxy_high))\n",
        "\n",
        "        if maxx_dec > 0.96 and miny_dec < 0.04:\n",
        "            files.append(generate_file_name(maxx_high, miny_low))\n",
        "\n",
        "        return files\n",
        "\n",
        "    CWs_dict = []\n",
        "    PC_list = []\n",
        "\n",
        "    # Loop over all crosswalks\n",
        "    for CW in CWs.iterrows():\n",
        "        # Get the polygon of the crosswalk\n",
        "        polygon = CW[1]['geometry']\n",
        "        \n",
        "        # Get min and max coordinates of polygon\n",
        "        minx, miny, maxx, maxy = polygon.bounds\n",
        "\n",
        "        # Divide by 50 to be in accordance with PC file names and round the bounds\n",
        "        rounded_bounds = [int(minx / 50), int(miny / 50), int(maxx / 50), int(maxy / 50)]\n",
        "        \n",
        "        # Generate base file names\n",
        "        files = [\n",
        "            generate_file_name(rounded_bounds[0], rounded_bounds[1]),\n",
        "            generate_file_name(rounded_bounds[0], rounded_bounds[3]),\n",
        "            generate_file_name(rounded_bounds[2], rounded_bounds[1]),\n",
        "            generate_file_name(rounded_bounds[2], rounded_bounds[3])\n",
        "        ]\n",
        "\n",
        "        # Calculate decimal parts to check for boundary conditions\n",
        "        minx_dec = minx / 50 - rounded_bounds[0]\n",
        "        miny_dec = miny / 50 - rounded_bounds[1]\n",
        "        maxx_dec = maxx / 50 - rounded_bounds[2]\n",
        "        maxy_dec = maxy / 50 - rounded_bounds[3]\n",
        "\n",
        "        # Add adjacent files if boundaries are close to the PC border\n",
        "        files = add_adjacent_files(files, rounded_bounds, minx_dec, miny_dec, maxx_dec, maxy_dec)\n",
        "\n",
        "        # Remove duplicates by converting to set and back to list\n",
        "        files = list(set(files))\n",
        "\n",
        "        CW_dict = {\n",
        "            'CW_index': CW[0],\n",
        "            'CW_polygon': polygon,\n",
        "            'PC_list': files\n",
        "        }\n",
        "\n",
        "        CWs_dict.append(CW_dict)\n",
        "        PC_list.extend(files)\n",
        "    \n",
        "    PC_list = list(set(PC_list))\n",
        "\n",
        "    return CWs_dict, PC_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "CWs, PC_list = find_PC_files(CW_polygons, PC_XYs, PC_file_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1716834958474
        }
      },
      "outputs": [],
      "source": [
        "# Function to load the PC files\n",
        "def load_PCs(PC_list, folder):\n",
        "    PCs = []\n",
        "    \n",
        "    for pc_name in PC_list:\n",
        "        file = os.path.join(folder, pc_name + \".laz\")\n",
        "        if os.path.exists(file):\n",
        "            laz_file = laspy.read(file)\n",
        "            name = pc_name.split(\".\")[0]\n",
        "            PC_coords = laz_file.xyz\n",
        "            PC_intensity = laz_file.intensity\n",
        "\n",
        "            PCs.append({\"name\": name, \"laz_file\": laz_file, \"PC_coords\": PC_coords, \"PC_intensity\": PC_intensity})\n",
        "        else: \n",
        "            print(file, \"does not exist\")\n",
        "           \n",
        "    return PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1716835165406
        }
      },
      "outputs": [],
      "source": [
        "# Load the PC files into a dictionary\n",
        "PCs = load_PCs(PC_list, PC_location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Down sampling and cutting point clouds\n",
        "\n",
        "As we are only interested in points that are on the ground, we can cut points above a certain threshold from the point clouds. Doing so will speed up further processing. \n",
        " Additionally, we will donwsample the point clouds. This is because we do not need the information from every point in the point cloud in order to obtain good results. Down sampling will also improve the processing time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1716835209442
        }
      },
      "outputs": [],
      "source": [
        "# Function to cut points in the point clouds that are above a certain threshold\n",
        "def cut_PC(pc):\n",
        "\n",
        "    # Find coordinates below threshold\n",
        "    indices = np.where(pc['PC_coords'][:, 2] < 5)\n",
        "\n",
        "    # Cut intensity accordingly\n",
        "    pc['PC_intensity_low'] = pc['PC_intensity'][indices]\n",
        "    pc['PC_coords_low'] = pc['PC_coords'][indices]\n",
        "\n",
        "    # Take the mean\n",
        "    mean = np.mean(pc['PC_coords_low'][:, 2])\n",
        "\n",
        "    # Cut again based on one std away from the mean\n",
        "    std_deviation = np.std(pc['PC_coords_low'][:, 2])\n",
        "\n",
        "    # Determine new threshold \n",
        "    threshold = mean + std_deviation\n",
        "\n",
        "    # Find coordinates below new threshold\n",
        "    indices_new = np.where(pc['PC_coords_low'][:, 2] < threshold)\n",
        "\n",
        "    # Cut intensity accordingly\n",
        "    pc['PC_intensity_low'] = pc['PC_intensity_low'][indices_new]\n",
        "    pc['PC_coords_low'] = pc['PC_coords_low'][indices_new]\n",
        "\n",
        "    return pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1716835212449
        }
      },
      "outputs": [],
      "source": [
        "# Function to downsample the PCs\n",
        "def down_sample_PC(pc, coords, intensity_string):\n",
        "    xyz = pc[coords]\n",
        "    intensity = pc[intensity_string]\n",
        "    \n",
        "    # Convert to Open3D point cloud using only XYZ\n",
        "    pc_o3d = o3d.geometry.PointCloud()\n",
        "    pc_o3d.points = o3d.utility.Vector3dVector(xyz)\n",
        "\n",
        "    # Perform voxel downsampling\n",
        "    downsampled_pc_o3d = pc_o3d.voxel_down_sample(0.02)\n",
        "\n",
        "    # Retrieve downsampled XYZ points\n",
        "    pc[coords + \"_ds\"] = np.asarray(downsampled_pc_o3d.points) \n",
        "\n",
        "    # Create a KDTree for the original point cloud\n",
        "    tree = cKDTree(xyz)\n",
        "\n",
        "    # For each downsampled point, find its nearest neighbor in the original cloud\n",
        "    _, indices = tree.query(pc[coords + '_ds'])\n",
        "\n",
        "    # Get indices of intensity\n",
        "    pc[intensity_string + \"_ds\"] = intensity[indices]\n",
        "\n",
        "    return pc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1716835216622
        }
      },
      "outputs": [],
      "source": [
        "# Function to process the PCs\n",
        "def cut_ds_PC(PCs, coord_string, intensity_string):\n",
        "    PCs_cut = []\n",
        "\n",
        "    for pc in PCs:\n",
        "        pc = cut_PC(pc)\n",
        "        pc = down_sample_PC(pc, coord_string, intensity_string)\n",
        "        PCs_cut.append(pc)\n",
        "    \n",
        "    return PCs_cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1716835651343
        }
      },
      "outputs": [],
      "source": [
        "# Downsample and cut the polygons based on height\n",
        "PCs_cut = cut_ds_PC(PCs, \"PC_coords_low\", \"PC_intensity_low\")\n",
        "del PCs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matching CW polygons to PCs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1716835752436
        }
      },
      "outputs": [],
      "source": [
        "# Cut point clouds based on polygon coordinate\n",
        "def PC_pol_match(PC, pol):\n",
        "\n",
        "    # Get the bounding box (rectangle) of the polygon\n",
        "    minx, miny, maxx, maxy = pol['CW_polygon'].bounds\n",
        "    \n",
        "    # Determine condition based on polygon bounds\n",
        "    condition = ((PC['PC_coords_low_ds'][:, 0] > minx) & (PC['PC_coords_low_ds'][:, 0] < maxx) \n",
        "                &  (PC['PC_coords_low_ds'][:, 1] > miny) & (PC['PC_coords_low_ds'][:, 1] < maxy))\n",
        "\n",
        "    # Apply condition to get indices\n",
        "    indexes = np.where(condition)\n",
        "\n",
        "    # Check if any matches were found\n",
        "    if len(indexes[0]) > 0:\n",
        "        \n",
        "        # Apply indexing to coordinates and intensity\n",
        "        intensity = PC['PC_intensity_low_ds'][indexes]\n",
        "        coords = PC['PC_coords_low_ds'][indexes]\n",
        "\n",
        "        return {'CW_index': pol['CW_index'], 'polygon': pol['CW_polygon'], 'PC_file': [PC['name']], 'PC_coords_low_ds': coords, 'PC_intensity_low_ds': intensity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1716835759452
        }
      },
      "outputs": [],
      "source": [
        "# Function to merge polygons that are spread over two point clouds\n",
        "def merge_matches(match1, match2):\n",
        "\n",
        "    # Concatenate coordinates and intensity of both PC files belonging to the same polygon \n",
        "    coords = np.vstack((match1['PC_coords_low_ds'], (match2['PC_coords_low_ds'])))\n",
        "    intensity = np.hstack((match1['PC_intensity_low_ds'], (match2['PC_intensity_low_ds'])))\n",
        "    \n",
        "    # Create list of PC files to add to dictionary \n",
        "    PC_list = match1['PC_file'] + match2['PC_file']\n",
        "    \n",
        "    # Create dictionary for matched point clouds\n",
        "    new_match = {'CW_index': match1['CW_index'], 'polygon': match1['polygon'], 'PC_file': PC_list, 'PC_coords_low_ds': coords, 'PC_intensity_low_ds': intensity}\n",
        "    \n",
        "    return new_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1716835765436
        }
      },
      "outputs": [],
      "source": [
        "# Function to group PC matches of the same polygon together\n",
        "def group_matches(all_matches):\n",
        "    # Create list to group together polygons that are spread over multiple point clouds\n",
        "    grouped_data = []\n",
        "\n",
        "    # Create a deep copy of the previously identified matches\n",
        "    match_copy = copy.deepcopy(all_matches)\n",
        "\n",
        "    # Loop over all matches\n",
        "    for item in match_copy:\n",
        "\n",
        "        index = item['CW_index']\n",
        "\n",
        "        found = False\n",
        "\n",
        "        for sublist in grouped_data:\n",
        "\n",
        "            # Check if the polygon is already in the list and append to the corresponding list item if this is the case\n",
        "            if sublist and sublist[0]['CW_index'] == index:\n",
        "                sublist.append(item)\n",
        "                found = True\n",
        "                break\n",
        "            \n",
        "        # If the polygon is not already in the list, append it \n",
        "        if not found:\n",
        "            grouped_data.append([item])\n",
        "    \n",
        "    return grouped_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1716835769422
        }
      },
      "outputs": [],
      "source": [
        "# Function to process PC matches of the same polygon\n",
        "def process_grouped_matches(grouped_data):\n",
        "    # Loop over the grouped polygons\n",
        "    for group in grouped_data:\n",
        "\n",
        "        # Check if there is multiple PC files for one polygon \n",
        "        if len(group) > 1:\n",
        "\n",
        "            # Loop over each item except the last one\n",
        "            for i in range(len(group) - 1):\n",
        "\n",
        "                # Merge the first item with the next one and replace the first item \n",
        "                match = merge_matches(group[0], group[1])\n",
        "                \n",
        "                group[0] = match\n",
        "\n",
        "                group.pop(1)\n",
        "\n",
        "    # Flatten the grouped data list as each list item only has one item now\n",
        "    grouped_data_flat = [item for sublist in grouped_data for item in sublist]\n",
        "\n",
        "    return grouped_data_flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1716835776664
        }
      },
      "outputs": [],
      "source": [
        "# Function to match PCs and polygons\n",
        "def match_PC_pol(CW_polygons, PCs):\n",
        "\n",
        "    # Create list to save all matches found\n",
        "    all_matches = []\n",
        "\n",
        "    # Loop over all polygons\n",
        "    for index in range(0, len(CW_polygons)):\n",
        "        cw = CW_polygons[index]\n",
        "\n",
        "        for pc in PCs:\n",
        "            if pc['name'] in cw['PC_list']:\n",
        "                match = PC_pol_match(pc, cw)\n",
        "                if match:\n",
        "                    all_matches.append(match)\n",
        "    \n",
        "    grouped_data = group_matches(all_matches)\n",
        "    merged_data = process_grouped_matches(grouped_data)\n",
        "    \n",
        "    return merged_data, grouped_data, all_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1716834001899
        }
      },
      "outputs": [],
      "source": [
        "# Find the PC data that matches the polygons\n",
        "merged_data, grouped_data, all_matches = match_PC_pol(CWs, PCs_cut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Growing polygons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To filter the polygons, we take several steps.\n",
        "1. Cluster areas within a polygon with a high intensity value in close proximity together \n",
        "2. For each cluster, check the surrounding points. If points have a high intensity value, add them to the cluster.\n",
        "\n",
        "Step 2 is repeated untill there are no more surrounding points with a high intensity value. This way, only areas that are present in the original polygon are grown and outside areas are not included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1716834001949
        }
      },
      "outputs": [],
      "source": [
        "# Function to filter the PC points in a polygon based on the intensity values of the points\n",
        "def filter_intensity(polygon, og_intensity, og_coords, new_intensity, new_coords):\n",
        "    return_polygon = copy.deepcopy(polygon)\n",
        "\n",
        "    # Determine condition based on polygon bounds\n",
        "    condition = (return_polygon[og_intensity] > 26000)\n",
        "\n",
        "    # Apply condition to get indices\n",
        "    indexes = np.where(condition)\n",
        "\n",
        "    # Check if any matches were found\n",
        "    if len(indexes[0]) > 0:\n",
        "        \n",
        "        # Apply indexing to coordinates and intensity\n",
        "        return_polygon[new_intensity] = return_polygon[og_intensity][indexes]\n",
        "        return_polygon[new_coords] = return_polygon[og_coords][indexes]\n",
        "       \n",
        "\n",
        "        return return_polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1716834001966
        }
      },
      "outputs": [],
      "source": [
        "# Function to cluster the polygon into clusters of points that have a high intensity and are close together in space\n",
        "def cluster_pol(CW):\n",
        "    # Create list to save the clusters that are found\n",
        "    cluster_list = []\n",
        "\n",
        "    # Filter the original polygon to only include points with a high intensity\n",
        "    filtered = filter_intensity(CW, \"PC_intensity_low_ds\", \"PC_coords_low_ds\", \"PC_intensity_low_ds_filtered\", \"PC_coords_low_ds_filtered\")\n",
        "    \n",
        "\n",
        "    if filtered:\n",
        "    \n",
        "        # Use DBSCAN to cluster the points in the polygon\n",
        "        dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
        "        dbscan.fit(filtered['PC_coords_low_ds_filtered'])\n",
        "\n",
        "        # Get labels created by DBSCAN\n",
        "        labels = dbscan.labels_\n",
        "\n",
        "        # Create dictionary to save clusters\n",
        "        cluster_data = {}\n",
        "\n",
        "        # Loop over each point in the filtered polygon and check to which cluster it belongs\n",
        "        # Group coordinates and intensity values based on their label in the cluster_data dictionary\n",
        "        for label, point, value in zip(labels, filtered['PC_coords_low_ds_filtered'], filtered['PC_intensity_low_ds_filtered']):\n",
        "            if label not in cluster_data:\n",
        "                cluster_data[label] = {'coords': [], 'intensity': []}  \n",
        "            cluster_data[label]['coords'].append(point)\n",
        "            cluster_data[label]['intensity'].append(value)\n",
        "        \n",
        "        # Transform the coordinates and intensity values to np arrays to make them easier to work with\n",
        "        for label in np.unique(labels):\n",
        "            cluster_data[label]['coords'] = np.array(cluster_data[label]['coords'])\n",
        "            cluster_data[label]['intensity'] = np.array(cluster_data[label]['intensity'])\n",
        "\n",
        "        # Loop over the created clusters and save them in the cluster_list\n",
        "        for cluster in cluster_data:\n",
        "            cluster_dict = {}\n",
        "\n",
        "            # Only keep clusters that are over 100 points to pre-emptively filter out noise\n",
        "            if (len(cluster_data[cluster]['coords']) > 50):\n",
        "\n",
        "                # Save cluster in a similar manner as the original polygon\n",
        "                cluster_dict['CW_index'] = CW['CW_index']\n",
        "                cluster_dict['PC_file'] = CW['PC_file']\n",
        "                cluster_dict['coordinates'] = cluster_data[cluster]['coords']\n",
        "                cluster_dict['intensity'] = cluster_data[cluster]['intensity']\n",
        "                \n",
        "                cluster_list.append(cluster_dict)\n",
        "        \n",
        "        return cluster_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1716834002001
        }
      },
      "outputs": [],
      "source": [
        "# Function to grow the clusters based on creating a buffer around them and seeing if there are points in the buffer with a high intensity\n",
        "def grow_cluster(PC_coords, PC_intensity, cluster_coords):\n",
        "\n",
        "    # Initialize an empty list to keep track of the points that are added to the clusters\n",
        "    added = []\n",
        "\n",
        "    # Build a KDTree for fast nearest neighbor search\n",
        "    tree = cKDTree(PC_coords)\n",
        "\n",
        "    # Define the radius within which points are considered neighbors\n",
        "    radius = 0.12\n",
        "\n",
        "    # Initialize the starting coordinates for the cluster growth as the original cluster\n",
        "    coords = cluster_coords\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # Find indices of neighbors within the specified radius\n",
        "        neighbor_indices = tree.query_ball_point(coords, radius)\n",
        "\n",
        "        # Initialize a list to store unique inidces of new points to add\n",
        "        indices = []\n",
        "\n",
        "        # Iterate through the neighbor indices to see if points have already been added\n",
        "        for index in neighbor_indices:\n",
        "            for i in index:\n",
        "                # Add them if this is not the case\n",
        "                if i not in added:\n",
        "                    indices.append(i)\n",
        "                    added.append(i)\n",
        "\n",
        "        # Remove duplicates from the lists of indices\n",
        "        indices = list(set(indices))\n",
        "        added = list(set(added))\n",
        "\n",
        "        # If no new points are found, exit the loop\n",
        "        if len(indices) == 0:\n",
        "            break\n",
        "\n",
        "        # Retrieve coordinates and intensities of the neighboring points\n",
        "        neighbor_coords = PC_coords[indices]\n",
        "        neighbor_intensities = PC_intensity[indices]\n",
        "\n",
        "        # Store the neighbors in a temporary dictionary\n",
        "        temp = {'coords': neighbor_coords, 'intensity': neighbor_intensities}\n",
        "\n",
        "        # Apply a filtering function to the temporary dictionary to only keep points with a high intensity\n",
        "        temp_filtered = filter_intensity(temp, \"intensity\", \"coords\", \"intensity_filtered\", \"coords_filtered\")\n",
        "        \n",
        "        if temp_filtered:\n",
        "            # If new filtered coordinates are available, update the coordinates for the next iteration\n",
        "            if 'coords_filtered' in temp_filtered:\n",
        "                coords = temp_filtered['coords_filtered']\n",
        "\n",
        "    # Extract the final cluster coordinates and intensities\n",
        "    cluster_coords = PC_coords[added]\n",
        "    cluster_intensity = PC_intensity[added]\n",
        "\n",
        "    # Store the final cluster information in a dictionary\n",
        "    final = {'coords': cluster_coords, 'intensity': cluster_intensity}\n",
        "\n",
        "    # Apply filtering to the final cluster\n",
        "    final = filter_intensity(final, \"intensity\", \"coords\", \"intensity_filtered\", \"coords_filtered\")\n",
        "    \n",
        "    # Return the filtered final cluster\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1716834002021
        }
      },
      "outputs": [],
      "source": [
        "# Function to create and process the PC polygon points\n",
        "def get_clusters(polygon, PCs, PC_coords_string, PC_intensity_string): \n",
        "\n",
        "    # Initialize coordinate and intensity array\n",
        "    PC_coords_temp = []\n",
        "    PC_intensity_temp = []\n",
        "\n",
        "    # Get PC file that corresponds to that of the original polygon\n",
        "    for PC_name in polygon['PC_file']:\n",
        "        PC = list(filter(lambda PC: PC['name'] == PC_name, PCs))\n",
        "        \n",
        "        sub_PC_coords = PC[0][PC_coords_string]\n",
        "        sub_PC_intensity = PC[0][PC_intensity_string]\n",
        "        \n",
        "        PC_coords_temp.append(sub_PC_coords)\n",
        "        PC_intensity_temp.append(sub_PC_intensity)\n",
        "    \n",
        "    \n",
        "    PC_coords = np.concatenate(PC_coords_temp, axis=0)\n",
        "    PC_intensity = np.concatenate(PC_intensity_temp, axis=0)\n",
        "\n",
        "    # Get clusters from polygon\n",
        "    cluster_dict = cluster_pol(polygon)\n",
        "\n",
        "    if cluster_dict:\n",
        "\n",
        "        # For each found cluster, grow it and update the cluster data\n",
        "        for cluster in cluster_dict:\n",
        "            clean_cluster = grow_cluster(PC_coords, PC_intensity, cluster['coordinates'])\n",
        "            if 'coords_filtered' in clean_cluster:\n",
        "                cluster['clean_coords'] = clean_cluster['coords_filtered']\n",
        "                cluster['clean_intensity'] = clean_cluster['intensity_filtered']\n",
        "        \n",
        "        # Return cluster dictionary\n",
        "        return cluster_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1716834002060
        }
      },
      "outputs": [],
      "source": [
        "def get_cluster_dict(merged_data, polygon_origin):\n",
        "\n",
        "    # Set path to save cluster dictionary based on polygons\n",
        "    if polygon_origin == \"T2N\":\n",
        "        path = \"../data/output/cluster dict.pkl\"\n",
        "    if polygon_origin == \"extension\":\n",
        "        path = \"../data/output/extension cluster dict.pkl\"\n",
        "\n",
        "    # Cluster the PC polygons and grow them to get complete road markings\n",
        "    final = []\n",
        "    too_long = []\n",
        "    for merge in merged_data:\n",
        "        try:\n",
        "            with timeout(3600, exception=RuntimeError):\n",
        "\n",
        "                print(\"working on\", merge['CW_index'])\n",
        "                cluster_dict = get_clusters(merge, PCs_cut, 'PC_coords_low_ds', 'PC_intensity_low_ds')\n",
        "\n",
        "                if cluster_dict:\n",
        "                    print(\"found clusters for\", merge['CW_index'])\n",
        "                    final.append(cluster_dict)\n",
        "\n",
        "                    with open(path, 'wb') as file:\n",
        "                        pickle.dump(final, file)\n",
        "                else:\n",
        "                    print(\"did not find clusters\")\n",
        "\n",
        "        except RuntimeError:\n",
        "            print(merge['CW_index'], \"took too long\")\n",
        "            too_long.append(merge['CW_index'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_cluster_dict(merged_data, polygon_origin)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
